# AIME Model API Server Configuration File
# Copyright (c) AIME GmbH and affiliates. Find more info at https://www.aime.info/api
#
# This software may be used and distributed according to the terms of the MIT LICENSE

[SERVER]
# basic HTTP server configuration
port = 7777
host = "0.0.0.0"
endpoint_configs = "./endpoints" # search path or list of endpoint configuration files to load on startup

[SANIC]
access_log = false # Disable or enable access log
proxies_count = 1 # The number of proxy servers in front of the app
keep_alive_timeout = 10 # How long to hold a TCP connection open in seconds
keep_alive = true # Disables keep-alive when False
request_buffer_size = 65536 # Request buffer size in bytes before request is paused
request_max_size = 100000000 	# How big a request may be in bytes
request_max_header_size = 8192 	# How big a request header may be in bytes
request_timeout = 60 	# How long a request can take to arrive in seconds
response_timeout = 60 # How long a response can take to process in seconds
websocket_max_size 	= 1048576 # Maximum size for incoming messages in bytes
websocket_ping_interval = 20 #	A Ping frame is sent every ping_interval seconds.
websocket_ping_timeout = 20 # Connection is closed when Pong is not received after ping_timeout seconds
worker_manager_threshold = 5000 # Worker Manager threshold in decisecond until all sanic worker needs to started

[ADMIN]
# login credentials to admin backend
user = 'admin'
password = ''

[CLIENTS]
# client default api authorization keys (can be overwritten in Endpoint configuration)

default_job_inactivity_timeout = 120 # Time in seconds after the job state is set to 'lapsed' if no worker update arrives.
default_result_lifetime = 900 # Time in seconds the job results will be available to the client after completion.

default_authorization_keys = { "aime" = "6a17e2a5-b706-03cb-1a32-94b4a1df67da" }

default_provide_worker_meta_data = true	# default is false
default_client_request_limit = 0


[WORKERS]
default_auth_key = "5b07e305b50505ca2b3284b4ae5f65d7"
default_request_timeout = 60

[ENDPOINTS]
default_max_queue_length = 1000
default_max_time_in_queue = 3600 # Time in seconds in queue after job state will be set to 'lapsed'

[INPUTS]
# Allowed formats for all media inputs of certain types. Formats not listed here will be rejected, no matter the supported formats in endpoint config file
image.format = { allowed = [ "png", "jpeg", "webp", "bmp" , "tiff", "gif"] }
audio.format = { allowed = [ 'wav', 'mp3', 'ogg', 'webm', 'mp4' ] }

[OPENAI]
enable_v1_api = true
enable_chat_completions = true

# use this model in case on is requested which is not supported
fallback_model = "llama4-chat"

[OPENAI.MODELS]
'gpt-4.1' = { endpoint = 'llama3_chat', legacy_context_format = true, created_at = "1733511208", owned_by = "Meta Llama 3.3 open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'llama3-chat' = { endpoint = 'llama3_chat', legacy_context_format = true, created_at = "1733511208", owned_by = "Meta Llama 3.3 open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'llama3-r1-chat' = { endpoint = 'llama3_r1_chat', legacy_context_format = true, created_at = "1740423208", owned_by = "Meta/Deepseek Llama 3.3 R1 open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'llama4-chat' = { endpoint = 'llama4_chat', legacy_context_format = true, created_at = "1746298408", owned_by = "Meta Llama 4 open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'mistral-chat' = { endpoint = 'mistral_chat', legacy_context_format = false, created_at = "1742323824", owned_by = "Mistral.ai open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'mixtral-chat' = { endpoint = 'mixtral_chat', legacy_context_format = true, created_at = "1724093424", owned_by = "Mistral.ai open-model", default_system_prompt = 'You are a helpful assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

'devstral-chat' = { endpoint = 'devstral_chat', legacy_context_format = true, created_at = "1747853424", owned_by = "Mistral.ai open-model", default_system_prompt = 'You are a helpful developer assistant working for AIME (Artificial Intelligence Machine Engineers). Be honest, polite and correct.' }

[STATIC]
# mount static served files, path relativ to location of the configuration file
# supported compile types: "None" (default), "scss" (SCSS compiled CSS) and "md" (markdown, will be compiled to HTML)

# demo endpoints shared resources
'/model_api/js/model_api.js' = { file = './frontend/static/js/model_api.js' }
'/model_api/frontend' = { path = './frontend/static' }
'/model_api/css' = { path = './frontend/scss', compiled_path = './frontend/static/_compiled_/css', compile = 'scss' }


# README and Documentation
'/' = { file = './README.md', compile = 'md', compiled_path = './frontend/static/_compiled_/html', css_file = './docs/css/markdown.css' }
'/favicon.ico' = { file = './frontend/static/favicon.ico' }

'/docs' = { path = './docs/build/html/' }
'/docs/css' = { path = './docs/css/' }
'/docs/images' = { path = './docs/images' }
